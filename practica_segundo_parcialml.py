# -*- coding: utf-8 -*-
"""practica_segundo_parcialML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wgg4GYoK5GdWdEB7bIdkR0-6MF-kWhiB
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

import seaborn as sns
import warnings
warnings.filterwarnings("ignore",category=DeprecationWarning)

"""**1. Cargar datos**

se utilizo el conjunto de datos de artículos que contine articulos de las materias de .... en un archivo CSV
"""

docs= pd.read_csv("https://raw.githubusercontent.com/DayBS/Machine-learning/main/2do-dataset/train.csv")
docs.head(5)

"""**2. Limpieza de datos**

Para realizar la prediccion solo necesitaremos el texto de cada documento
"""

docs.columns=docs.columns.str.lower().str.replace(" ","_",regex=True)

docs.head(5)

docs.info()

docs = docs.drop(columns=[
'id','computer_science','mathematics','physics','statistics','analysis_of_pdes','applications',  
'artificial_intelligence','astrophysics_of_galaxies','computation_and_language','computer_vision_and_pattern_recognition',   
'cosmology_and_nongalactic_astrophysics','data_structures_and_algorithms','differential_geometry',             
'earth_and_planetary_astrophysics', 'fluid_dynamics', 'information_theory','instrumentation_and_methods_for_astrophysics',  
'machine_learning',          
'materials_science',
'methodology',              
'number_theory',                  
'optimization_and_control',       
'representation_theory',         
'robotics',            
'social_and_information_networks', 
'statistics_theory',                  
'strongly_correlated_electrons',              
'superconductivity',                      
'systems_and_control'                      
 ], axis=1)

docs.head()

docs.info()

"""se realizara un preprocesamiento simple en el contenido del abstract ‎‎para que los resultados del analisis sean mas confiables 
Para hacer eso, se usara una expresión regular para eliminar cualquier puntuación y luego ‎‎pondremos el texto en minúsculas‎‎.‎
"""

#libreria para usar expresiones regulares
import re
# Eliminar los caracteres como puntos, simbolos de dolar, comas, etc.
docs['docs_procesados'] = \
docs['abstract'].map(lambda x: re.sub('[$,\.!?_]', '', x))
# LLevar las palabras a minusculas
docs['docs_procesados'] = \
docs['docs_procesados'].map(lambda x: x.lower())
# Visualizar la columna de datos procesados
docs.head()

docs.head()

"""**3. Analisis Exploratorio**

‎Para verificar  el preprocesamiento, se hara uso de la libreria wordcloud, que nos permite tener una representacion visual de las palabras mas comunes.
nos permite comprender los datos y verificar si no es necesario realizar mas preprocesamiento antes de entrenar el modelo
"""

from wordcloud import WordCloud
#todos los resumenes en una sola lista
print(list(docs['docs_procesados'].values)[:2]) 
long_string = ','.join(list(docs['docs_procesados'].values))
print(long_string[:500])
# crear un objeto de la clase importada
wordcloud = WordCloud(background_color="white", max_words=5000, contour_width=3, contour_color='steelblue')
# generar la nube
wordcloud.generate(long_string)
# visualizar la nube
wordcloud.to_image()

"""**4. Preparar los datos para el analisis LDA**

*  Primero se eliminan las palabras comunes que poco influyen al momento de tomar una decisión.



*  posteriormente se convertira cada objeto Tokenizado en un corpus y un diccionario


"""

import gensim
from gensim.utils import simple_preprocess
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

#recuperar una lista de palabras de parada del idioma inglés
stop_words = stopwords.words('english')
#se agrega otras que no estan dentro del conjundo de palabras de parada
stop_words.extend(['from', 'subject', 're', 'edu', 'use'])
print(stop_words)

def sent_to_words(sentences):
  #recibir una lista de tokens de cada articulo
    for sentence in sentences:
      #usar la herramienta gensim para el procesamiento de los textos de cada articulo,
      #deacc en true para eliminar las puntuaciones
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))

def remove_stopwords(texts):
    return [[word for word in simple_preprocess(str(doc)) 
             if word not in stop_words] for doc in texts]

#obtener una lista que contenga el contenido de cada articulo
datos = docs.docs_procesados.values.tolist() ##[[articulo 1],[articulo2]]
#obtener las palabras.
palabras_documento= list(sent_to_words(datos))#[['','',''],['','','']]
print(palabras_documento[0:2])

# eliminar las palabras de parada
palabras_documento = remove_stopwords(palabras_documento)
print(palabras_documento[0:2])

#se eliminaran palabras como have,an,and,etc

import gensim.corpora as corpora
# creacion del diccionario donde cada palabra fue tokenizada
id2word= corpora.Dictionary(palabras_documento)
print(id2word)

# creacion del copus
texts = palabras_documento
corpus = [id2word.doc2bow(text) for text in texts]

print(corpus[:1][0][:30])

"""**Entrenamiento del modelo**

* los parametros alfa y beta seran asignados aleatoreamente

* el numero de temas sera de 4

* usaremos las palabras con el formato requerido por el modelo

** Nota: Cada documento es una combinacion de palabras 

Cada palabra contribuye un peso al tema **



"""

from pprint import pprint
# establecer el numero de topicos
num_topics = 4
# importar el modelo LDA con los datos,las palabras y el numero de topicos
lda_model = gensim.models.LdaMulticore(corpus=corpus,
                                       id2word=id2word,
                                       num_topics=num_topics)
# 
pprint(lda_model.print_topics())

doc_lda = lda_model[corpus]

print(doc_lda)

"""**Analisis de los resultado**

se usara la libreria pyLDAvis, el cual nos permite visualizar los resultados del entrenamiento del modelo 
* analisis de las palabras para cada tema
* analisis de relaciones entre los temas



"""

##instalacion de pyLDAvis
!pip install pyLDAvis
#!pip install gensim

import pyLDAvis.gensim_models as gensimvis
import pickle 
import pyLDAvis
import os

! touch ldavis_prepared

#visualizar los temas

pyLDAvis.enable_notebook()
LDAvis_data_filepath = os.path.join('./ldavis_prepared')

if 1 == 1:
    LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)
    with open(LDAvis_data_filepath,'wb') as f:
        pickle.dump(LDAvis_prepared, f)

with open(LDAvis_data_filepath, 'rb') as f:
    LDAvis_prepared = pickle.load(f)  

LDAvis_prepared

"""**5. Importacion del modelo**"""

from gensim.test.utils import datapath

# temp_file = datapath("model")
file_model = "./model"
lda_model.save(file_model)

lda_model_import2 =gensim.models.LdaMulticore.load(file_model)

print(lda_model)

print(lda_model_import2)

def predict_topic(text):
  #importar el modelo entrenado
  lda_model_import =gensim.models.LdaMulticore.load(file_model)
  test_corpus=id2word.doc2bow(text.split())

  return (lda_model_import[test_corpus])

examples= pd.read_csv("https://raw.githubusercontent.com/DayBS/Machine-learning/main/2do-dataset/test.csv")
examples.head()

examples.columns=examples.columns.str.lower().str.replace(" ","_",regex=True)

examples = examples.drop(columns=[
'id','computer_science','mathematics','physics','statistics'], axis=1)

# Eliminar los caracteres como puntos, simbolos de dolar, comas, etc.
examples['docs_procesados'] = \
examples['abstract'].map(lambda x: re.sub('[$,\.!?_]', '', x))
# LLevar las palabras a minusculas
examples['docs_procesados'] = \
examples['docs_procesados'].map(lambda x: x.lower())
# Visualizar la columna de datos procesados

examples.docs_procesados[64]

text="inside this paper we exploit the memory-augmented neural network to predict accurate answers to visual questions even when those answers occur rarely inside a training set a memory network incorporates both internal and external memory blocks and selectively pays attention to each training exemplar we show that memory-augmented neural networks are able to maintain the relatively long-term memory of scarce training exemplars which was important considering visual question answering due to a heavy-tailed distribution of answers inside the general vqa setting experimental results on two large-scale benchmark datasets show a favorable performance of a proposed algorithm with the comparison to state of a art"

predict_topic(text)

! pip install fitz
!pip install PyMuPDF

import cv2
import glob
import os
import fitz

def obtener_lista_archivos(ruta):
    archivos_pdf=glob.glob(ruta+"/*.pdf")
    return archivos_pdf

obtener_lista_archivos("/content/sample_data")

def leer_contenido_pdf(ruta_pdf):
    documento=fitz.open(ruta_pdf)
    pagina=documento.load_page(0)
    contenido=pagina.get_text("text")
    return contenido

leer_contenido_pdf("/content/sample_data/procedimiento.pdf")

def maximo(lista_tuplas):
  maximo= lista_tuplas[0][1]
  posicion_maximo=0
  for i in range(1,4):
    if(lista_tuplas[i][1]>maximo):
      maximo=lista_tuplas[i][1]
      print(maximo)
      posicion_maximo=i
  return i

def predecir_carpeta(contenido):
    lista_probabilidades=predict_topic(contenido)
    topico=maximo(lista_tuplas=lista_probabilidades)
    return topico

predecir_carpeta("1.\ninstalar conda(explicar que hace, porque es necesario)\n2.\nobtener el dataset(internet, videos de simulaciones)\n3.\nvariar las imágenes del dataset(blanco y negro, cerca, lejos, angulos)\n4.\npre procesamiento(normalizar el tamaño)\n5.\netiquetado(herramientas, formato coco)\n6.\nimportación de los datos\n7.\nentrenamiento\n8.\npruebas\n9.\nimplementacion real\nnota:instalar torch para cpu\n")

def asignar_carpeta(ruta,archivo,carpeta):
    ruta_archivo=archivo
    ruta_destino=ruta+"/"+str(carpeta)

    if not(os.path.isdir(ruta_destino)):
        print("no deberia")
        os.system("mkdir "+ruta_destino)
    print("mv "+ruta_archivo+" "+ruta_destino)
    os.system("mv "+ruta_archivo+" "+ruta_destino)

def ordenar_documentos(ruta):
    lista_pdfs=obtener_lista_archivos(ruta)
    for ruta_pdf in lista_pdfs:
        texto=leer_contenido_pdf(ruta_pdf)
        carpeta_destino=predecir_carpeta(texto)
        print(texto)
        print(carpeta_destino)
        asignar_carpeta(ruta,ruta_pdf,carpeta_destino)

ordenar_documentos("/content/sample_data")







!pip install googletrans==3.1.0a0

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from googletrans import Translator
import seaborn as sns
import warnings
warnings.filterwarnings("ignore",category=DeprecationWarning)

trans=Translator();

#traduccion
df['esp'] = df.apply(lambda row: trans.translate(row["docs_procesados"],dest="es").text , axis = 1) 
df.to_csv("train_traducido.csv")

"""**1. Cargar datos**

se utilizo el conjunto de datos de artículos que contine articulos de las materias de .... en un archivo CSV
"""

df= pd.read_csv("https://raw.githubusercontent.com/DayBS/Machine-learning/main/2do-dataset/train.csv")

df.columns=df.columns.str.lower().str.replace(" ","_",regex=True)
df = df.drop(columns=[
'id','computer_science','mathematics','physics','statistics','analysis_of_pdes','applications',  
'artificial_intelligence','astrophysics_of_galaxies','computation_and_language','computer_vision_and_pattern_recognition',   
'cosmology_and_nongalactic_astrophysics','data_structures_and_algorithms','differential_geometry',             
'earth_and_planetary_astrophysics', 'fluid_dynamics', 'information_theory','instrumentation_and_methods_for_astrophysics',  
'machine_learning',          
'materials_science',
'methodology',              
'number_theory',                  
'optimization_and_control',       
'representation_theory',         
'robotics',            
'social_and_information_networks', 
'statistics_theory',                  
'strongly_correlated_electrons',              
'superconductivity',                      
'systems_and_control'                      
 ], axis=1)

#libreria para usar expresiones regulares
import re
# Eliminar los caracteres como puntos, simbolos de dolar, comas, etc.
df['docs_procesados'] = \
df['abstract'].map(lambda x: re.sub('[$,\.!?_]', '', x))
# LLevar las palabras a minusculas
df['docs_procesados'] = \
df['docs_procesados'].map(lambda x: x.lower())
# Visualizar la columna de datos procesados

df=df.drop("abstract",axis=1)

"""**2. Limpieza de datos**

Para realizar la prediccion solo necesitaremos el texto de cada documento
"""

df['esp'] = df.apply(lambda row: trans.translate(row["docs_procesados"],dest="es").text , axis = 1)

df.head(100)



documentos= pd.read_csv("train_traducido.csv")
documentos.head(5)

documentos = documentos.drop(columns=["docs_procesados","Unnamed: 0"], axis=1)
documentos.head(5)

"""**3. Preparar los datos para el analisis LDA**

Primero se eliminan las palabras comunes que poco influyen al momento de tomar una decisión.
posteriormente se convertira cada objeto Tokenizado en un corpus y un diccionario
"""

import gensim
from gensim.utils import simple_preprocess
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

#recuperar una lista de palabras de parada del idioma español
stop_words = stopwords.words('spanish')
#se agrega otras que no estan dentro del conjundo de palabras de parada
stop_words.extend(['dentro','afuera','lejos','considerando','si','no','talvez','mas','menos','uno','dos','tres','cuatro','cinco','tambien'])
print(stop_words)

def sent_to_words(sentences):
  #recibir una lista de tokens de cada articulo
    for sentence in sentences:
      #usar la herramienta gensim para el procesamiento de los textos de cada articulo,
      #deacc en true para eliminar las puntuaciones
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))

def remove_stopwords(texts):
    return [[word for word in simple_preprocess(str(doc)) 
             if word not in stop_words] for doc in texts]

#obtener una lista que contenga el contenido de cada articulo
datos = documentos.esp.values.tolist() ##[[articulo 1],[articulo2]]
#obtener las palabras.
palabras_documento= list(sent_to_words(datos))#[['','',''],['','','']]
print(palabras_documento[0:2])

# eliminar las palabras de parada
palabras_documento = remove_stopwords(palabras_documento)
print(palabras_documento[0:2])

#se eliminaran palabras como have,an,and,etc

import gensim.corpora as corpora
# creacion del diccionario donde cada palabra fue tokenizada
id2word= corpora.Dictionary(palabras_documento)
print(id2word)

# creacion del copus
texts = palabras_documento
corpus = [id2word.doc2bow(text) for text in texts]

print(corpus[:1][0][:30])

"""**4. Entrenamiento del modelo**

los parametros alfa y beta seran asignados aleatoreamente

el numero de temas sera de 4

usaremos las palabras con el formato requerido por el modelo

** Nota: Cada documento es una combinacion de palabras

Cada palabra contribuye un peso al tema **
"""

from pprint import pprint
# establecer el numero de topicos
num_topics = 4
# importar el modelo LDA con los datos,las palabras y el numero de topicos
lda_model = gensim.models.LdaMulticore(corpus=corpus,
                                       id2word=id2word,
                                       num_topics=num_topics)
# 
pprint(lda_model.print_topics())

"""**5. Analisis de los resultado**

se usara la libreria pyLDAvis, el cual nos permite visualizar los resultados del entrenamiento del modelo

analisis de las palabras para cada tema
analisis de relaciones entre los temas
"""

!pip install pyLDAvis

! touch ldavis_prepared
pyLDAvis.enable_notebook()
LDAvis_data_filepath = os.path.join('./ldavis_prepared')

if 1 == 1:
    LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)
    with open(LDAvis_data_filepath,'wb') as f:
        pickle.dump(LDAvis_prepared, f)

with open(LDAvis_data_filepath, 'rb') as f:
    LDAvis_prepared = pickle.load(f)   
           
LDAvis_prepared

"""**6. exportacion del modelo**"""

from gensim.test.utils import datapath     

# temp_file = datapath("model")
file_model = "./modelo_entrenado"
lda_model.save(file_model)

"""**importacion del modelo**

"""

modelo_entrenado_importado =gensim.models.LdaMulticore.load("./modelo_entrenado") 
print(modelo_entrenado_importado)

examples= pd.read_csv("https://raw.githubusercontent.com/DayBS/Machine-learning/main/2do-dataset/test.csv")
examples=examples[0:1000]

examples.columns=examples.columns.str.lower().str.replace(" ","_",regex=True)

examples = examples.drop(columns=[
'id','computer_science','mathematics','physics','statistics'], axis=1)

# Eliminar los caracteres como puntos, simbolos de dolar, comas, etc.
examples['docs_procesados'] = \
examples['abstract'].map(lambda x: re.sub('[$,\.!?_]', '', x))
# LLevar las palabras a minusculas
examples['docs_procesados'] = \
examples['docs_procesados'].map(lambda x: x.lower())
# Visualizar la columna de datos procesados

examples['esp'] = examples.apply(lambda row: trans.translate(row["docs_procesados"],dest="es").text , axis = 1)
examples.head(100)

examples=examples.drop(columns=['abstract','docs_procesados'],axis=1)
examples.head(50)

examples.to_csv("test_traducido.csv")

examples.docs_procesados[64]

#text="inside this paper we exploit the memory-augmented neural network to predict accurate answers to visual questions even when those answers occur rarely inside a training set a memory network incorporates both internal and external memory blocks and selectively pays attention to each training exemplar we show that memory-augmented neural networks are able to maintain the relatively long-term memory of scarce training exemplars which was important considering visual question answering due to a heavy-tailed distribution of answers inside the general vqa setting experimental results on two large-scale benchmark datasets show a favorable performance of a proposed algorithm with the comparison to state of a art"
text="se trata de un sistema inteligente con capacidad de entender el habla común, pero requiere de cientos a miles de datos para poder realizar la tarea de aprendizaje del modelo"
predict_topic(text)

! pip install fitz
!pip install PyMuPDF

"""**Implementacion del modelo para la organizacion de documentos**"""

import cv2
import glob
import os
import fitz

def obtener_lista_archivos(ruta):
    archivos_pdf=glob.glob(ruta+"/*.pdf")
    return archivos_pdf

def leer_contenido_pdf(ruta_pdf):
    documento=fitz.open(ruta_pdf)
    pagina=documento.load_page(0)
    contenido=pagina.get_text("text")
    return contenido


def maximo(lista_tuplas):
  print(lista_tuplas)
  maximo= lista_tuplas[0][1]
  posicion_maximo=0
  for i in range(1,4):
    if(lista_tuplas[i][1]>maximo):
      maximo=lista_tuplas[i][1]
      print(maximo)
      posicion_maximo=i
      
  return posicion_maximo

def predecir_carpeta(contenido):
    lista_probabilidades=predict_topic(contenido)
    topico=maximo(lista_tuplas=lista_probabilidades)
    return topico
 
def asignar_carpeta(ruta,archivo,carpeta):
    ruta_archivo=archivo
    ruta_destino=ruta+str(carpeta)

    if not(os.path.isdir(ruta_destino)):
        print("no deberia")
        os.system("mkdir "+ruta_destino)
    print("mv "+"'"+ruta_archivo+"'"+" "+ruta_destino)
    os.system("mv "+ruta_archivo+" "+ruta_destino)


def ordenar_documentos(ruta):
    lista_pdfs=obtener_lista_archivos(ruta)
    for ruta_pdf in lista_pdfs:
        texto=leer_contenido_pdf(ruta_pdf)
        carpeta_destino=predecir_carpeta(texto)
        print(texto)
        print(carpeta_destino)
        asignar_carpeta(ruta,ruta_pdf,carpeta_destino)

ordenar_documentos("/content/carpeta/")









